# AI ChatBot ğŸ¤–

An intelligent AI-powered chatbot application that can assist with any topic - from technology and science to business and everyday questions. Built with Python Flask, Ollama (local LLM), Docker, and Kubernetes.

![AI ChatBot](https://img.shields.io/badge/AI-ChatBot-blue)
![Python](https://img.shields.io/badge/Python-3.11-green)
![Docker](https://img.shields.io/badge/Docker-Ready-blue)
![Kubernetes](https://img.shields.io/badge/Kubernetes-Ready-blue)

## ğŸŒŸ Features

- **AI-Powered Responses**: Uses Ollama with Llama 3.1 8B for intelligent assistance across any topic (100% free, runs locally)
- **Modern UI**: Clean, responsive chat interface with smooth animations and glassmorphism effects
- **Conversation History**: Maintains context across multiple messages
- **General Knowledge**: Can discuss technology, science, business, arts, health, and much more
- **Production Ready**: Containerized with Docker and deployable to Kubernetes
- **Health Checks**: Built-in health endpoints for monitoring
- **Security**: Non-root container user, environment variable management
- **No API Keys**: Runs completely locally with Ollama - no external API dependencies
- **High Quality Responses**: 8B parameter model provides detailed, helpful answers

## ğŸ› ï¸ Tech Stack

### Backend
- **Python 3.11** - Application runtime
- **Flask** - Web framework
- **Ollama** - Local LLM server (Llama 3.1 8B model - 4.9GB)
- **Gunicorn** - Production WSGI server

### Frontend
- **HTML5/CSS3** - Modern responsive design
- **JavaScript (ES6+)** - Interactive chat functionality
- **Font Awesome** - Icons

### DevOps
- **Docker** - Containerization
- **Docker Compose** - Local orchestration
- **Kubernetes** - Production orchestration
- **GitHub Actions** - CI/CD pipeline
- **Trivy** - Security scanning

## ğŸš€ Quick Start

### Prerequisites

- Python 3.11+
- Docker & Docker Compose
- At least 4GB of RAM (for Ollama)

### Local Development

1. **Clone the repository**:
   ```bash
   git clone https://github.com/alexmachulsky/ai-chatbot.git
   cd ai-chatbot
   ```

2. **Set up environment variables**:
   ```bash
   cp .env.example .env
   # Edit .env to customize Ollama settings if needed
   ```

3. **Install dependencies**:
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   pip install -r requirements.txt
   ```

4. **Run the application**:
   ```bash
   python app.py
   ```

5. **Access the chatbot**:
   Open your browser and navigate to `http://localhost:5000`

### Docker Compose Deployment (Recommended for Local/Testing)

1. **Build and run with Docker Compose** (includes Ollama):
   ```bash
   docker compose up -d
   ```

   This will:
   - Start the Ollama server container
   - Pull the Llama 3.1 8B model (first run takes 2-3 minutes)
   - Start the Flask chatbot application

2. **Pull the Ollama model** (if not already pulled):
   ```bash
   docker exec ollama-server ollama pull llama3.1:8b
   ```

3. **Access the chatbot**:
   Open `http://localhost:5000`

4. **View logs**:
   ```bash
   docker compose logs -f chatbot
   docker compose logs -f ollama
   ```

5. **Stop the containers**:
   ```bash
   docker compose down
   ```

### Kubernetes Deployment (Production-Ready)

The chatbot supports full Kubernetes deployment with high availability and scalability features.

#### Prerequisites
- Kubernetes cluster (tested with Minikube v1.35.0)
- kubectl configured
- Docker for building images
- Ollama running (via Docker Compose recommended)

#### Quick Deploy

1. **Start Ollama** (via Docker Compose for better resource management):
   ```bash
   docker-compose up -d ollama
   docker exec ollama-server ollama pull llama3.1:8b
   ```

2. **Build the image** (for Minikube):
   ```bash
   eval $(minikube docker-env)
   docker build -t ai-chatbot:latest .
   ```

3. **Deploy to Kubernetes**:
   ```bash
   cd k8s
   kubectl apply -f configmap.yaml
   kubectl apply -f deployment.yaml
   kubectl apply -f service.yaml
   ```

4. **Wait for pods to be ready**:
   ```bash
   kubectl wait --for=condition=ready pod -l app=devops-chatbot --timeout=60s
   ```

5. **Access the application**:
   ```bash
   # Via port-forward (development)
   kubectl port-forward svc/devops-chatbot 8888:80
   # Access at http://localhost:8888
   
   # Via NodePort (network access)
   minikube service devops-chatbot --url
   ```

#### Kubernetes Features

- **High Availability**: 2 replicas with automatic failover
- **Health Checks**: Liveness and readiness probes
- **Resource Management**: CPU and memory limits
- **Session Affinity**: Maintains user sessions
- **Scalability**: Easy horizontal scaling with `kubectl scale`
- **Rolling Updates**: Zero-downtime deployments

#### Monitoring
```bash
# Check pod status
kubectl get pods -l app=devops-chatbot

# View logs
kubectl logs -l app=devops-chatbot -f

# Check resource usage
kubectl top pods -l app=devops-chatbot
```

For detailed Kubernetes deployment instructions, see [k8s/README.md](k8s/README.md)

## ğŸ“‹ API Endpoints

### POST /api/chat
Send a message to the chatbot.

**Request**:
```json
{
  "message": "What is photosynthesis?",
  "history": []
}
```

**Response**:
```json
{
  "message": "Photosynthesis is the process by which...",
  "success": true
}
```

### GET /api/health
Health check endpoint for monitoring.

**Response**:
```json
{
  "status": "healthy",
  "service": "devops-chatbot"
}
```

## ğŸ¯ Use Cases

The AI ChatBot can help with:

- **Technology & Programming**: Coding questions, debugging, best practices, frameworks
- **Science & Mathematics**: Explanations, problem-solving, research concepts
- **Business & Finance**: Strategy, analysis, market insights, career advice
- **Arts & Literature**: Creative writing, analysis, recommendations
- **Health & Wellness**: General information, fitness tips, nutrition basics
- **Education**: Learning assistance, explanations, study help
- **General Knowledge**: History, geography, current events, and more

## ğŸ”§ Configuration

### Environment Variables

| Variable | Description | Required | Default |
|----------|-------------|----------|---------|
| `OLLAMA_URL` | Ollama server URL | No | `http://ollama:11434` |
| `OLLAMA_MODEL` | Ollama model to use | No | `llama3.2:1b` |
| `FLASK_ENV` | Flask environment | No | `production` |
| `FLASK_DEBUG` | Enable Flask debug mode | No | `False` |
| `PORT` | Port to run the server | No | `5000` |
| `HOST` | Host to bind the server | No | `0.0.0.0` |
| `SECRET_KEY` | Flask secret key | No | Auto-generated |

### Available Ollama Models

You can change the model by updating the `OLLAMA_MODEL` environment variable:

- `llama3.1:8b` - **CURRENT & RECOMMENDED** (4.9GB, best balance of quality and performance)
- `llama3.2:3b` - Smaller, faster (2GB, good for limited RAM)
- `llama3.2:1b` - Smallest, fastest (1.3GB, basic quality)
- `llama2` - Original model (3.8GB, older version)
- `codellama` - Code-specialized model (7B, 4.1GB)

To switch models:
```bash
# Pull a different model
docker exec ollama-server ollama pull llama3.2:3b

# Update .env file
OLLAMA_MODEL=llama3.2:3b

# Restart chatbot
docker compose restart chatbot
```

## ğŸ”’ Security

- Non-root container user for enhanced security
- Environment variables for sensitive data
- CORS configuration for API access control
- Health check endpoints for monitoring
- Trivy security scanning in CI/CD pipeline
- Resource limits in Kubernetes deployment

## ğŸ“Š CI/CD Pipeline

The GitHub Actions pipeline automatically:

1. **Lints** Python code with flake8 and black
2. **Builds** Docker image
3. **Tests** the container
4. **Scans** for security vulnerabilities with Trivy
5. **Uploads** security results to GitHub Security
6. **Notifies** deployment readiness

## ğŸ¨ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   User Browser  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Frontend (HTML â”‚
â”‚   CSS, JS)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Flask Backend   â”‚
â”‚ (Python 3.11)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ollama Server   â”‚
â”‚ Llama 3.1 8B    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Project Structure

```
ai-chatbot/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci-cd.yaml          # CI/CD pipeline
â”œâ”€â”€ k8s/
â”‚   â”œâ”€â”€ configmap.yaml          # Kubernetes ConfigMap
â”‚   â”œâ”€â”€ deployment.yaml         # Kubernetes Deployment
â”‚   â”œâ”€â”€ secret.yaml.example     # Secret template
â”‚   â””â”€â”€ service.yaml            # Kubernetes Service
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ index.html              # Frontend HTML
â”‚   â”œâ”€â”€ style.css               # Styling
â”‚   â””â”€â”€ script.js               # Chat functionality
â”œâ”€â”€ .dockerignore               # Docker ignore rules
â”œâ”€â”€ .env.example                # Environment variables template
â”œâ”€â”€ .gitignore                  # Git ignore rules
â”œâ”€â”€ app.py                      # Flask application
â”œâ”€â”€ docker-compose.yml          # Docker Compose config
â”œâ”€â”€ Dockerfile                  # Container image definition
â”œâ”€â”€ README.md                   # This file
â””â”€â”€ requirements.txt            # Python dependencies
```

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“„ License

This project is open source and available under the [MIT License](LICENSE).

## ğŸ™ Acknowledgments

- [Ollama](https://ollama.ai/) for providing free, local LLM inference
- [Meta AI](https://ai.meta.com/) for the Llama 3.1 model
- Flask framework and community
- Docker and Kubernetes communities
- Font Awesome for icons

## ğŸ“§ Contact

- **Email**: alexm051197@gmail.com
- **GitHub**: [@alexmachulsky](https://github.com/alexmachulsky)
- **LinkedIn**: [Alex Machulsky](https://www.linkedin.com/in/alex-machulsky-)

---

**Built with â¤ï¸ by Alex Machulsky | DevOps Engineer**
